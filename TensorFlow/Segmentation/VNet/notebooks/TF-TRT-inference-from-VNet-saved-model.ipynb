{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-TRT Inference from VNet Model Checkpoint with TF 1.15\n",
    "\n",
    "In this notebook, we demonstrate the process to create a TF-TRT optimized model from a Tensorflow VNet model checkpoint.\n",
    "\n",
    "This notebook was designed to run with TensorFlow version 1.15 which is included as part of NVIDIA NGC Tensorflow containers from `nvcr.io/nvidia/tensorflow:19.11-py3` to `nvcr.io/nvidia/tensorflow:19.12-tf1-py3` that can be downloaded from the [NGC website](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow).\n",
    " \n",
    "\n",
    "## Notebook  Content\n",
    "1. [Pre-requisite: data and model](#1)\n",
    "1. [Verifying the orignal FP32 model](#2)\n",
    "1. [Creating TF-TRT FP32 model](#3)\n",
    "1. [Creating TF-TRT FP16 model](#4)\n",
    "1. [Creating TF-TRT INT8 model](#5)\n",
    "1. [Calibrating TF-TRT INT8 model with raw JPEG images](#6)\n",
    " \n",
    "## Quick start\n",
    "We will run this demonstration with a saved VNet model to be downloaded from https://ngc.nvidia.com/catalog/models/nvidia:vnettf_fp32.\n",
    "\n",
    "The INT8 calibration process requires access to a small but representative sample of real training or valiation data.\n",
    "\n",
    "We will use the Hippocampus dataset from [medical segmentation decathlon](http://medicaldecathlon.com/). Test images provided by the organization were used to produce the resulting masks for submission.\n",
    "\n",
    "\n",
    "To run this notebook, follow the instruction at https://ngc.nvidia.com/catalog/model-scripts/nvidia:vnet_medical_for_tensorflow/quickStartGuide to build the required Docker container and download the Hippocampus dataset.\n",
    "\n",
    "Start the NGC TF container, providing correct path to the Hippocampus dataset `/path/to/dataset` (if already downloaded):\n",
    "\n",
    "```bash\n",
    " docker run --runtime=nvidia --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -v /path/to/dataset:/data vnet_tf:latest bash\n",
    "```\n",
    "\n",
    "Within the container, we then start Jupyter notebook with:\n",
    "\n",
    "```bash\n",
    "jupyter notebook --ip 0.0.0.0 --port 8888  --allow-root\n",
    "```\n",
    "\n",
    "Connect to Jupyter notebook web interface on your host http://localhost:8888.\n",
    "\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## 1. Pre-requisite: data and model\n",
    "\n",
    "We first import some required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.15.0\n",
      "TensorRT version: \n",
      "ii  libnvinfer-bin                         6.0.1-1+cuda10.2                  amd64        TensorRT binaries\n",
      "ii  libnvinfer-dev                         6.0.1-1+cuda10.2                  amd64        TensorRT development libraries and headers\n",
      "ii  libnvinfer-plugin-dev                  6.0.1-1+cuda10.2                  amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer-plugin6                     6.0.1-1+cuda10.2                  amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer6                            6.0.1-1+cuda10.2                  amd64        TensorRT runtime libraries\n",
      "ii  python3-libnvinfer                     6.0.1-1+cuda10.2                  amd64        Python 3 bindings for TensorRT\n",
      "ii  python3-libnvinfer-dev                 6.0.1-1+cuda10.2                  amd64        Python 3 development package for TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# check TensorRT version\n",
    "print(\"TensorRT version: \")\n",
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "If not already downloaded, the Hippocampus dataset from [medical segmentation decathlon](http://medicaldecathlon.com/) can be downloaded with the download_dataset.py script. It is possible to select the destination folder when downloading the files by using the --data_dir flag. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C into ./data/Task04_Hippocampus.tar... Done.\n",
      "Unpacking...\n",
      "Traceback (most recent call last):\n",
      "  File \"../download_dataset.py\", line 61, in <module>\n",
      "    main()\n",
      "  File \"../download_dataset.py\", line 50, in main\n",
      "    tf = tarfile.open(os.path.join(FLAGS.data_dir, filename))\n",
      "  File \"/usr/lib/python3.6/tarfile.py\", line 1576, in open\n",
      "    raise ReadError(\"file could not be opened successfully\")\n",
      "tarfile.ReadError: file could not be opened successfully\n"
     ]
    }
   ],
   "source": [
    "!python ../download_dataset.py --data_dir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.json  imagesTr\timagesTs  labelsTr\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/Task04_Hippocampus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [paper](https://arxiv.org/pdf/1902.09063.pdf):\n",
    "\n",
    "\"The dataset consisted of MRI acquired in 90 healthy adults and 105 adults with a non-affective psychotic disorder (56 schizophrenia, 32 schizoaffective disorder, and 17 schizophreniform disorder) taken from the Psychiatric Genotype/Phenotype Project data repository at Vanderbilt University Medical Center (Nashville, TN, USA). Patients were recruited from the Vanderbilt Psychotic Disorders Program and controls were recruited from the surrounding community.  All participants were assessed with the Structured Clinical Interview for DSM-IV [15]. All subjects were free from significant med-ical or neurological illness, head injury, and active substance use or dependence.\n",
    "\n",
    "Structural images were acquired with a 3D T1-weighted MPRAGE sequence(TI/TR/TE, 860/8.0/3.7 ms; 170 sagittal slices; voxel size, 1.0 mm3). All images were collected on a Philips Achieva scanner (Philips Healthcare, Inc., Best, The Netherlands). Manual tracing of the head, body, and tail of the hippocampus on images was completed following a previously published protocol [16, 17]. For the purposes of this dataset, the term hippocampus includes the hippocampus proper (CA1-4 and dentate gyrus) and parts of the subiculum, which together are more often termed the hippocampal formation [18]. The last slice of the head of the hippocampus was defined as the coronal slice containing theuncal apex. The resulting 195 labeled images are referred to as hippocampus atlases. Note that the term hippocampus posterior refers to the union of the body and the tail.\"\n",
    "\n",
    "Next, let's inspect the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/workspace/vnet/')\n",
    "from utils.data_loader import MSDDataset\n",
    "\n",
    "from utils.cmd_util import PARSER\n",
    "FLAGS = PARSER.parse_args([\"--exec_mode=predict\",\"--batch_size=1\", \"--model_dir=./vnet_model\",\n",
    "                           \"--data_dir=./data/Task04_Hippocampus\"])\n",
    "\n",
    "dataset = MSDDataset(json_path=os.path.join('./data/Task04_Hippocampus/dataset.json'),\n",
    "                     dst_size=FLAGS.input_shape,\n",
    "                     seed=FLAGS.seed,\n",
    "                     interpolator=FLAGS.resize_interpolator,\n",
    "                     data_normalization=FLAGS.data_normalization,\n",
    "                     batch_size=FLAGS.batch_size,\n",
    "                     train_split=0,\n",
    "                     split_seed=FLAGS.split_seed)\n",
    "\n",
    "FLAGS.labels = dataset.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data\n",
    "There are 260 3-D training images of size 32 x 32 x 32, each with a segmentation label of the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images:  260\n",
      "Data:  (1, 32, 32, 32)\n",
      "Label:  (1, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import horovod.tensorflow as hvd\n",
    "hvd.init()\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    test_data = dataset.eval_fn()\n",
    "    iterator = test_data.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    cnt = 0\n",
    "    try:\n",
    "        while True:        \n",
    "            image_data = sess.run(next_element)    \n",
    "            cnt += 1\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        pass\n",
    "    print(\"Total number of training images: \", cnt)\n",
    "    print(\"Data: \", image_data[0].shape)\n",
    "    print(\"Label: \",image_data[1].shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show one slide of a training image and associated labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_id = 0\n",
    "slide_id = 10\n",
    "\n",
    "plt.figure(figsize = (10,10));\n",
    "plt.imshow(image_data[0][img_id][slide_id], cmap='gray');\n",
    "plt.title(\"1 slide of a 3-D training volume\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,10));\n",
    "plt.imshow(image_data[1][img_id][slide_id], cmap='gray');\n",
    "plt.title(\"Label for 1 slide of a 3-D training volume\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data\n",
    "There are 130 3-D test images of size 32 x 32 x 32 without segmentation labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test images:  130\n",
      "Data shape:  (1, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    test_data = dataset.test_fn()\n",
    "    iterator = test_data.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    cnt = 0\n",
    "    try:\n",
    "        while True:        \n",
    "            image_data = sess.run(next_element)    \n",
    "            cnt += 1\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        pass\n",
    "    print(\"Total number of test images: \", cnt)\n",
    "    print(\"Data shape: \", image_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VNet model checkpoint\n",
    "If not already downloaded, we will be downloading and working with a VNet pretrained model from  https://ngc.nvidia.com/catalog/models/nvidia:vnettf_fp32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./vnet_model.zip\n",
      "  inflating: vnet_model/checkpoint   \n",
      "  inflating: vnet_model/eval/events.out.tfevents.1574423527.941577  \n",
      "  inflating: vnet_model/events.out.tfevents.1574423303.941577  \n",
      "  inflating: vnet_model/graph.pbtxt  \n",
      "  inflating: vnet_model/joblog.log   \n",
      "  inflating: vnet_model/model.ckpt-9360.data-00000-of-00001  \n",
      "  inflating: vnet_model/model.ckpt-9360.index  \n",
      "  inflating: vnet_model/model.ckpt-9360.meta  \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "wget -nc -q --show-progress -O vnet_model.zip \\\n",
    "https://api.ngc.nvidia.com/v2/models/nvidia/vnettf_fp32/versions/1/zip\n",
    "unzip -o ./vnet_model.zip -d vnet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Verifying the orignal VNet model checkpoint\n",
    "First, we inspect the original VNet model loaded from the checkpoint. First, we set a global inference BATCH_SIZE we will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "FLAGS.batch_size = BATCH_SIZE\n",
    "\n",
    "dataset = MSDDataset(json_path=os.path.join('./data/Task04_Hippocampus/dataset.json'),\n",
    "                     dst_size=FLAGS.input_shape,\n",
    "                     seed=FLAGS.seed,\n",
    "                     interpolator=FLAGS.resize_interpolator,\n",
    "                     data_normalization=FLAGS.data_normalization,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     train_split=0,\n",
    "                     split_seed=FLAGS.split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_fn import vnet_v2\n",
    "\n",
    "gpu_options = tf.GPUOptions()\n",
    "config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "config.gpu_options.allow_growth = True\n",
    "    \n",
    "run_config = tf.estimator.RunConfig(\n",
    "    save_summary_steps=None,\n",
    "    save_checkpoints_steps=None,\n",
    "    save_checkpoints_secs=None,\n",
    "    tf_random_seed=None,\n",
    "    session_config=config,\n",
    "    keep_checkpoint_max=1)\n",
    "    \n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=vnet_v2,\n",
    "    model_dir=FLAGS.model_dir,\n",
    "    config=run_config,\n",
    "    params=FLAGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's carry out prediction on a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = estimator.predict(input_fn=lambda: dataset.test_fn(count=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [p['prediction'] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to saved model format\n",
    "As TF-TRT works with saved model format, we shall now export the pretrained model to a saved model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type = tf.float32\n",
    "\n",
    "SAVED_MODEL_DIR = './saved_model'\n",
    "!rm -rf $SAVED_MODEL_DIR\n",
    "\n",
    "def get_serving_input_receiver_fn():\n",
    "\n",
    "    def serving_input_receiver_fn():\n",
    "        features = tf.placeholder(dtype=input_type, shape=(BATCH_SIZE, 32, 32, 32), name='input_tensor')\n",
    "\n",
    "        return tf.estimator.export.TensorServingInputReceiver(features=features, receiver_tensors=features)\n",
    "\n",
    "    return serving_input_receiver_fn\n",
    "\n",
    "export_path = estimator.export_saved_model(\n",
    "    export_dir_base='%s'%SAVED_MODEL_DIR,\n",
    "    serving_input_receiver_fn=get_serving_input_receiver_fn(),\n",
    "    checkpoint_path='./vnet_model/model.ckpt-9360'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ `saved_model_cli` to inspect the inputs and outputs of the model. Note that `estimator.export_saved_model` will create a random subdirectory within `SAVED_MODEL_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579143644\r\n"
     ]
    }
   ],
   "source": [
    "!ls $SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "subdir = os.popen('ls %s'%SAVED_MODEL_DIR).read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1579143644'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 03:00:47.071724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (8, 32, 32, 32)\n",
      "        name: input_tensor:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['prediction'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (8, 32, 32, 32)\n",
      "        name: vnet/ArgMax:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "SAVED_MODEL_DIR = os.path.join(SAVED_MODEL_DIR, subdir)\n",
    "!saved_model_cli show --all --dir $SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./saved_model/1579143644'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This give us information on the input and output tensors as `input_tensor:0` and `softmax_tensor:0` respectively. Also note that the number of output classes here is 1001 instead of 1000 Imagenet classes. This is because the network was trained with an extra background class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TENSOR = 'input_tensor:0'\n",
    "OUTPUT_TENSOR = 'vnet/ArgMax:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to read in a saved mode, measuring its speed and accuracy on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_saved_model(SAVED_MODEL_DIR, BATCH_SIZE=1):\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        dataset = MSDDataset(json_path=os.path.join('./data/Task04_Hippocampus/dataset.json'),\n",
    "                     dst_size=FLAGS.input_shape,\n",
    "                     seed=FLAGS.seed,\n",
    "                     interpolator=FLAGS.resize_interpolator,\n",
    "                     data_normalization=FLAGS.data_normalization,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     train_split=0,\n",
    "                     split_seed=FLAGS.split_seed)\n",
    "        \n",
    "        # prepare dataset iterator\n",
    "        test_data = dataset.test_fn()\n",
    "        test_data = test_data.repeat()\n",
    "        iterator = test_data.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        tf.saved_model.loader.load(\n",
    "            sess, [tf.saved_model.tag_constants.SERVING], SAVED_MODEL_DIR)\n",
    "        \n",
    "        node_list = [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "        \n",
    "        print('Warming up for 50 batches...')\n",
    "        for _ in range (50):\n",
    "            sess.run(OUTPUT_TENSOR, feed_dict={INPUT_TENSOR: sess.run(next_element)})\n",
    "\n",
    "        print('Benchmarking inference engine for 1000 batches...')\n",
    "        num_predict = 0\n",
    "        start_time = time.time()\n",
    "        for _ in range (1000):       \n",
    "                data = sess.run(next_element)\n",
    "                sess.run(OUTPUT_TENSOR, feed_dict={INPUT_TENSOR: data})\n",
    "                num_predict += data.shape[0]\n",
    "                \n",
    "        print('Inference speed: %.2f samples/s'%(num_predict/(time.time()-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine for 1000 batches...\n",
      "Inference speed: 770.01 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(SAVED_MODEL_DIR, BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Creating TF-TRT FP32 model\n",
    "\n",
    "Next, we convert the native TF FP32 model to TF-TRT FP32, then verify model accuracy and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine for 1000 batches...\n",
      "Inference speed: 669.60 samples/s\n"
     ]
    }
   ],
   "source": [
    "FP32_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"_TFTRT_FP32/1\"\n",
    "!rm -rf $FP32_SAVED_MODEL_DIR\n",
    "\n",
    "# Now we create the TFTRT FP32 engine\n",
    "converter = trt.TrtGraphConverter(input_saved_model_dir=SAVED_MODEL_DIR,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.FP32)\n",
    "converter.convert()\n",
    "converter.save(FP32_SAVED_MODEL_DIR)\n",
    "\n",
    "benchmark_saved_model(FP32_SAVED_MODEL_DIR, BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Creating TF-TRT FP16 model\n",
    "\n",
    "Next, we convert the native TF FP32 model to TF-TRT FP16, then verify model accuracy and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine for 1000 batches...\n",
      "Inference speed: 1050.69 samples/s\n"
     ]
    }
   ],
   "source": [
    "FP16_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"_TFTRT_FP16/1\"\n",
    "!rm -rf $FP16_SAVED_MODEL_DIR\n",
    "\n",
    "# Now we create the TFTRT FP16 engine\n",
    "converter = trt.TrtGraphConverter(input_saved_model_dir=SAVED_MODEL_DIR,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.FP16)\n",
    "converter.convert()\n",
    "converter.save(FP16_SAVED_MODEL_DIR)\n",
    "\n",
    "benchmark_saved_model(FP16_SAVED_MODEL_DIR, BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Creating TF-TRT INT8 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-TRT INT8 inference model requires two steps:\n",
    "\n",
    "- Step 1: Prepare a calibration dataset\n",
    "\n",
    "- Step 2: Convert and calibrate the TF-TRT INT8 inference engine\n",
    "\n",
    "### Step 1: Prepare a calibration dataset\n",
    "\n",
    "Creating TF-TRT INT8 model requires a small calibration dataset. This data set ideally should represent the test data in production well, and will be used to create a value histogram for each layer in the neural network for effective 8-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data shape:  (16, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 2\n",
    "batched_input = np.zeros((BATCH_SIZE * num_calibration_batches, 32, 32, 32), dtype=np.float32)\n",
    "\n",
    "with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "    # prepare dataset iterator\n",
    "    dataset = MSDDataset(json_path=os.path.join('./data/Task04_Hippocampus/dataset.json'),\n",
    "                 dst_size=FLAGS.input_shape,\n",
    "                 seed=FLAGS.seed,\n",
    "                 interpolator=FLAGS.resize_interpolator,\n",
    "                 data_normalization=FLAGS.data_normalization,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 train_split=0,\n",
    "                 split_seed=FLAGS.split_seed)\n",
    "\n",
    "    # prepare dataset iterator\n",
    "    test_data = dataset.test_fn()\n",
    "    test_data = test_data.repeat()\n",
    "    iterator = test_data.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    for i in range(num_calibration_batches):\n",
    "        batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :] = sess.run(next_element)\n",
    "\n",
    "print('Calibration data shape: ', batched_input.shape)\n",
    "\n",
    "def calibration_input_fn_gen():\n",
    "    for i in range(num_calibration_batches):\n",
    "        yield batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "        \n",
    "calibration_input_fn = calibration_input_fn_gen()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Convert and calibrate the TF-TRT INT8 inference engine\n",
    "\n",
    "The calibration step may take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a directory to write the saved model\n",
    "INT8_SAVED_MODEL_DIR =  SAVED_MODEL_DIR + \"_TFTRT_INT8/1\"\n",
    "!rm -rf $INT8_SAVED_MODEL_DIR\n",
    "    \n",
    "converter = trt.TrtGraphConverter(\n",
    "      input_saved_model_dir=SAVED_MODEL_DIR,\n",
    "      precision_mode=trt.TrtPrecisionMode.INT8)\n",
    "  \n",
    "converter.convert()\n",
    "\n",
    "# Run calibration for num_calibration_batches times.\n",
    "converted_graph_def = converter.calibrate(\n",
    "      fetch_names=[OUTPUT_TENSOR],\n",
    "      num_runs=num_calibration_batches,\n",
    "      feed_dict_fn=lambda: {INPUT_TENSOR: next(calibration_input_fn)})\n",
    "\n",
    "converter.save(INT8_SAVED_MODEL_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking INT8 saved model\n",
    "\n",
    "Finally we reload and verify the accuracy and performance of the INT8 saved model from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine for 1000 batches...\n",
      "Inference speed: 1041.42 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(INT8_SAVED_MODEL_DIR, BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 03:07:02.754401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (8, 32, 32, 32)\n",
      "        name: input_tensor:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['prediction'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (8, 32, 32, 32)\n",
      "        name: vnet/ArgMax:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir $INT8_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have demonstrated the process of creating TF-TRT inference model from an original pretrained VNet model checkpoint. In every case, we have also verified the accuracy and speed to the resulting model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
