{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "TensorFlow_UNet_Industrial_Colab_train_and_inference.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinhngx/DeepLearningExamples/blob/vinhn_unet_industrial_demo/TensorFlow/Segmentation/UNet_Industrial/notebooks/TensorFlow_UNet_Industrial_Colab_train_and_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwt7z7qdmTbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4NKCp2VmTbn",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
        "\n",
        "# UNet Industrial Training and Inference Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW0OKDzvmTbt",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This U-Net model is adapted from the original version of the [U-Net model](https://arxiv.org/abs/1505.04597) which is\n",
        "a convolutional auto-encoder for 2D image segmentation. U-Net was first introduced by\n",
        "Olaf Ronneberger, Philip Fischer, and Thomas Brox in the paper:\n",
        "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).\n",
        "\n",
        "This work proposes a modified version of U-Net, called `TinyUNet` which performs efficiently and with very high accuracy\n",
        "on the industrial anomaly dataset [DAGM2007](https://resources.mpi-inf.mpg.de/conference/dagm/2007/prizes.html).\n",
        "*TinyUNet*, like the original *U-Net* is composed of two parts:\n",
        "- an encoding sub-network (left-side)\n",
        "- a decoding sub-network (right-side).\n",
        "\n",
        "It repeatedly applies 3 downsampling blocks composed of two 2D convolutions followed by a 2D max pooling\n",
        "layer in the encoding sub-network. In the decoding sub-network, 3 upsampling blocks are composed of a upsample2D\n",
        "layer followed by a 2D convolution, a concatenation operation with the residual connection and two 2D convolutions.\n",
        "\n",
        "`TinyUNet` has been introduced to reduce the model capacity which was leading to a high degree of over-fitting on a\n",
        "small dataset like DAGM2007. The complete architecture is presented in the figure below:\n",
        "\n",
        "![UnetModel](https://github.com/vinhngx/DeepLearningExamples/blob/vinhn_unet_industrial_demo/TensorFlow/Segmentation/UNet_Industrial/images/unet.png?raw=1)\n",
        "\n",
        "\n",
        "\n",
        "### Learning objectives\n",
        "\n",
        "This notebook demonstrates the steps for training a UNet model. We then employ the trained model to make inference on new images.\n",
        "\n",
        "## Content\n",
        "1. [Requirements](#1)\n",
        "1. [Data download and preprocessing](#2)\n",
        "1. [Training](#3)\n",
        "1. [Testing trained model](#4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDFrE4eqmTbv",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "## 1. Requirements\n",
        "\n",
        "\n",
        "### 1.1 Docker container\n",
        "The most convenient way to make use of the NVIDIA Mask R-CNN model is via a docker container, which provides a self-contained, isolated and re-producible environment for all experiments. Refer to the [Quick Start Guide section](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN#requirements) of the Readme documentation for a comprehensive guide. We briefly summarize the steps here.\n",
        "\n",
        "First, clone the repository:\n",
        "\n",
        "```\n",
        "git clone https://github.com/NVIDIA/DeepLearningExamples.git\n",
        "cd DeepLearningExamples/PyTorch/Segmentation/MaskRCNN\n",
        "```\n",
        "\n",
        "Next, build the NVIDIA Mask R-CNN container:\n",
        "\n",
        "```\n",
        "cd pytorch\n",
        "docker build --rm -t nvidia_joc_maskrcnn_pt .\n",
        "```\n",
        "\n",
        "Then launch the container with:\n",
        "\n",
        "```\n",
        "PATH_TO_COCO='/path/to/coco-2014'\n",
        "MOUNT_LOCATION='/datasets/data'\n",
        "NAME='nvidia_maskrcnn'\n",
        "\n",
        "docker run --it --runtime=nvidia -p 8888:8888 -v $PATH_TO_COCO:/$MOUNT_LOCATION --rm --name=$NAME --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --ipc=host nvidia_joc_maskrcnn_pt\n",
        "```\n",
        "where `/path/to/coco-2014` is the path on the host machine where the data was/is to be downloaded. More on data set preparation in the next section.\n",
        "\n",
        "Within the docker interactive bash session, start Jupyter with\n",
        "\n",
        "```\n",
        "jupyter notebook --ip 0.0.0.0 --port 8888\n",
        "```\n",
        "\n",
        "Then open the Jupyter GUI interface on your host machine at http://localhost:8888. Within the container, this notebook itself is located at `/workspace/object_detection/demo`.\n",
        "\n",
        "### 1.2 Hardware\n",
        "This notebook can be executed on any CUDA-enabled NVIDIA GPU, although for efficient mixed precision training, a [Tensor Core NVIDIA GPU](https://www.nvidia.com/en-us/data-center/tensorcore/) is desired (Volta, Turing or newer architectures). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7RLEcKhmTb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqSUGePjmTb9",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "This notebook demonstrates training and validation of the Mask R-CNN model on the [COCO 2014 dataset](http://cocodataset.org/#download). If not already available locally, the following [script](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Segmentation/MaskRCNN/download_dataset.sh) in the repository provides a convenient way to download and extract all the necessary data in one go. Be mindful of the size of the raw data (~20GB). The script makes use of `wget` and will automatically resume if disrupted. Once downloaded, the script invokes `dtrx` to extract the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2PR7weWmTcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget https://raw.githubusercontent.com/NVIDIA/DeepLearningExamples/master/PyTorch/Segmentation/MaskRCNN/download_dataset.sh -P /workspace/object_detection/\n",
        "! bash /workspace/object_detection/download_dataset.sh /datasets/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQAIszkxmTcT",
        "colab_type": "text"
      },
      "source": [
        "Within the docker container, the final data directory should look like:\n",
        "\n",
        "```\n",
        "/datasets/data\n",
        "  annotations/\n",
        "    instances_train2014.json\n",
        "    instances_val2014.json\n",
        "  train2014/\n",
        "    COCO_train2014_*.jpg\n",
        "  val2014/\n",
        "    COCO_val2014_*.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8d9IwzmTcV",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "## 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6wayGf1mTcX",
        "colab_type": "text"
      },
      "source": [
        "The shell script [train.sh](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Segmentation/MaskRCNN/pytorch/scripts/train.sh) provides a convenient interface to launch training tasks. \n",
        "By default, invoking [train.sh](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Segmentation/MaskRCNN/pytorch/scripts/train.sh) will make use of 8 GPUs, saves checkpoints every 2500 iterations and uses mixed precision training.\n",
        "\n",
        "```\n",
        "cd /workspace/object_detection/\n",
        "bash scripts/train.sh\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQPsExf6mTca",
        "colab_type": "text"
      },
      "source": [
        "Note that, within [train.sh](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Segmentation/MaskRCNN/pytorch/scripts/train.sh), it invokes the following Python command:\n",
        "\n",
        "```python -m torch.distributed.launch --nproc_per_node=8 tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" DTYPE \"float16\"```\n",
        "\n",
        "which launches pytorch distributed training with 8 GPUs, using the train script in [tools/train_net.py](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Segmentation/MaskRCNN/pytorch/tools/train_net.py). Various sample training configurations are available within the [configs](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN/pytorch/configs) directory, for example, a [configuration file](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Segmentation/MaskRCNN/pytorch/configs/e2e_mask_rcnn_R_50_FPN_1x_1GPU.yaml) for training using 1 GPU. \n",
        "\n",
        "### 3.1 Training with 1 GPU\n",
        "We will now take a closer look at training a Mask-RCNN model using 1 GPU, using the below custom config script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HapDsY4VmTce",
        "colab_type": "text"
      },
      "source": [
        "#### Training with full precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tob3-qvomTcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run -m torch.distributed.launch -- --nproc_per_node=8 ../tools/train_net.py --config-file \"/workspace/object_detection/configs/custom_config_8_GPUs.yml\" DTYPE \"float32\" OUTPUT_DIR ./results/8GPU-FP32/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzBPzaSGmTcl",
        "colab_type": "text"
      },
      "source": [
        "If the Jupyter graphical interface does not update the training progress on the fly, you can observe the information being printed in the shell window from where you launched Jupyter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-aFEwb4mTcn",
        "colab_type": "text"
      },
      "source": [
        "#### Training with mixed-precision\n",
        "We now launch the training process using mixed precision. Observe the information being printed in the shell window from where you launched Jupyter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3AZ-CXYmTcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run -m torch.distributed.launch -- --nproc_per_node=8 ../tools/train_net.py --config-file \"/workspace/object_detection/configs/custom_config_8_GPUs.yml\" DTYPE \"float16\" OUTPUT_DIR ./results/8GPU-FP16/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X959LYwjmTcw",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"4\"></a>\n",
        "## 4. Testing trained model\n",
        "\n",
        "After model training has completed, we can test the trained model against the COCO-2014 validation set. First, we create a new configuration file for the test. Note: you must point the model `WEIGHT` parameter to a final model checkpoint, e.g. `./results/8GPU-FP32/model_final.pth`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "TURMzc6HmTcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run ../tools/test_net.py \\\n",
        "    --config-file /workspace/object_detection/configs/test_custom_config.yml \\\n",
        "    DTYPE \"float16\" \\\n",
        "    DATASETS.TEST \"(\\\"coco_2014_minival\\\",)\" \\\n",
        "    OUTPUT_DIR ./results/8GPU-FP16/evaluation \\\n",
        "    TEST.IMS_PER_BATCH 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6oArfSmTc5",
        "colab_type": "text"
      },
      "source": [
        "### Testing on new images\n",
        "\n",
        "We will now launch an interactive testing, where you can load new test images. First, we load some required libraries and define some helper functions to load images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8MxXY5GmTc8",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we have walked through the complete process of preparing the container and data required for training Mask-RCNN models. We have also investigated various training options, trained and tested Mask-RCNN models with various configurations.\n",
        "\n",
        "## What's next\n",
        "Now it's time to try the MaskR-CNN on your own data. Observe the performance impact of mixed precision training while comparing the final accuracy of the models trained with FP32 and mixed precision.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "249yGNLmmTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}