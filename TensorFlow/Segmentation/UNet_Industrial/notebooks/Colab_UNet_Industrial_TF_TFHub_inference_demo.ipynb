{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Colab_UNet_Industrial_TF_TFHub_inference_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinhngx/DeepLearningExamples/blob/vinhn_unet_industrial_demo/TensorFlow/Segmentation/UNet_Industrial/notebooks/Colab_UNet_Industrial_TF_TFHub_inference_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gwt7z7qdmTbW",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4NKCp2VmTbn"
      },
      "source": [
        "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
        "\n",
        "# UNet Industrial Inference Demo with TensorFlow Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fW0OKDzvmTbt"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "In this notebook, we will demo the process of inference with  NVIDIA pre-trained UNet Industrial defects detection TensorFlow Hub modules.\n",
        "\n",
        "NVIDIA pre-trained U-Net models for defect detection are adapted from the original version of the [U-Net model](https://arxiv.org/abs/1505.04597) which is\n",
        "a convolutional auto-encoder for 2D image segmentation. U-Net was first introduced by\n",
        "Olaf Ronneberger, Philip Fischer, and Thomas Brox in the paper:\n",
        "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).\n",
        "\n",
        "### Requirement\n",
        "1. Before running this notebook, please set the Colab runtime environment to GPU via the menu *Runtime => Change runtime type => GPU*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HVsrGkj4Zn2L",
        "outputId": "e9e69477-9679-40ef-e12a-f47512391854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Oct 23 12:47:56 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pV3rzgO8-tSK"
      },
      "source": [
        "The below code checks whether a Tensor-Core GPU is present.  Tensor Cores can accelerate large matrix operations by performing mixed-precision matrix multiply and accumulate calculations in a single operation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Djyvo8mm9poq",
        "outputId": "7d5e0795-c33e-4979-ff45-3d1c5a672c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def check_tensor_core_gpu_present():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    for line in local_device_protos:\n",
        "        if \"compute capability\" in str(line):\n",
        "            compute_capability = float(line.physical_device_desc.split(\"compute capability: \")[-1])\n",
        "            if compute_capability>=7.0:\n",
        "                return True\n",
        "    \n",
        "print(\"Tensor Core GPU Present:\", check_tensor_core_gpu_present())\n",
        "tensor_core_gpu = check_tensor_core_gpu_present()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor Core GPU Present: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FCEfkBAbbaLI"
      },
      "source": [
        "2. Next, we clone the NVIDIA Github UNet_Industrial repository and set up the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y3u_VMjXtAto",
        "outputId": "8dc7df0e-0961-44d1-c299-3b0581df6740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "!git clone https://github.com/NVIDIA/DeepLearningExamples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearningExamples'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/164)\u001b[K\rremote: Counting objects:   1% (2/164)\u001b[K\rremote: Counting objects:   2% (4/164)\u001b[K\rremote: Counting objects:   3% (5/164)\u001b[K\rremote: Counting objects:   4% (7/164)\u001b[K\rremote: Counting objects:   5% (9/164)\u001b[K\rremote: Counting objects:   6% (10/164)\u001b[K\rremote: Counting objects:   7% (12/164)\u001b[K\rremote: Counting objects:   8% (14/164)\u001b[K\rremote: Counting objects:   9% (15/164)\u001b[K\rremote: Counting objects:  10% (17/164)\u001b[K\rremote: Counting objects:  11% (19/164)\u001b[K\rremote: Counting objects:  12% (20/164)\u001b[K\rremote: Counting objects:  13% (22/164)\u001b[K\rremote: Counting objects:  14% (23/164)\u001b[K\rremote: Counting objects:  15% (25/164)\u001b[K\rremote: Counting objects:  16% (27/164)\u001b[K\rremote: Counting objects:  17% (28/164)\u001b[K\rremote: Counting objects:  18% (30/164)\u001b[K\rremote: Counting objects:  19% (32/164)\u001b[K\rremote: Counting objects:  20% (33/164)\u001b[K\rremote: Counting objects:  21% (35/164)\u001b[K\rremote: Counting objects:  22% (37/164)\u001b[K\rremote: Counting objects:  23% (38/164)\u001b[K\rremote: Counting objects:  24% (40/164)\u001b[K\rremote: Counting objects:  25% (41/164)\u001b[K\rremote: Counting objects:  26% (43/164)\u001b[K\rremote: Counting objects:  27% (45/164)\u001b[K\rremote: Counting objects:  28% (46/164)\u001b[K\rremote: Counting objects:  29% (48/164)\u001b[K\rremote: Counting objects:  30% (50/164)\u001b[K\rremote: Counting objects:  31% (51/164)\u001b[K\rremote: Counting objects:  32% (53/164)\u001b[K\rremote: Counting objects:  33% (55/164)\u001b[K\rremote: Counting objects:  34% (56/164)\u001b[K\rremote: Counting objects:  35% (58/164)\u001b[K\rremote: Counting objects:  36% (60/164)\u001b[K\rremote: Counting objects:  37% (61/164)\u001b[K\rremote: Counting objects:  38% (63/164)\u001b[K\rremote: Counting objects:  39% (64/164)\u001b[K\rremote: Counting objects:  40% (66/164)\u001b[K\rremote: Counting objects:  41% (68/164)\u001b[K\rremote: Counting objects:  42% (69/164)\u001b[K\rremote: Counting objects:  43% (71/164)\u001b[K\rremote: Counting objects:  44% (73/164)\u001b[K\rremote: Counting objects:  45% (74/164)\u001b[K\rremote: Counting objects:  46% (76/164)\u001b[K\rremote: Counting objects:  47% (78/164)\u001b[K\rremote: Counting objects:  48% (79/164)\u001b[K\rremote: Counting objects:  49% (81/164)\u001b[K\rremote: Counting objects:  50% (82/164)\u001b[K\rremote: Counting objects:  51% (84/164)\u001b[K\rremote: Counting objects:  52% (86/164)\u001b[K\rremote: Counting objects:  53% (87/164)\u001b[K\rremote: Counting objects:  54% (89/164)\u001b[K\rremote: Counting objects:  55% (91/164)\u001b[K\rremote: Counting objects:  56% (92/164)\u001b[K\rremote: Counting objects:  57% (94/164)\u001b[K\rremote: Counting objects:  58% (96/164)\u001b[K\rremote: Counting objects:  59% (97/164)\u001b[K\rremote: Counting objects:  60% (99/164)\u001b[K\rremote: Counting objects:  61% (101/164)\u001b[K\rremote: Counting objects:  62% (102/164)\u001b[K\rremote: Counting objects:  63% (104/164)\u001b[K\rremote: Counting objects:  64% (105/164)\u001b[K\rremote: Counting objects:  65% (107/164)\u001b[K\rremote: Counting objects:  66% (109/164)\u001b[K\rremote: Counting objects:  67% (110/164)\u001b[K\rremote: Counting objects:  68% (112/164)\u001b[K\rremote: Counting objects:  69% (114/164)\u001b[K\rremote: Counting objects:  70% (115/164)\u001b[K\rremote: Counting objects:  71% (117/164)\u001b[K\rremote: Counting objects:  72% (119/164)\u001b[K\rremote: Counting objects:  73% (120/164)\u001b[K\rremote: Counting objects:  74% (122/164)\u001b[K\rremote: Counting objects:  75% (123/164)\u001b[K\rremote: Counting objects:  76% (125/164)\u001b[K\rremote: Counting objects:  77% (127/164)\u001b[K\rremote: Counting objects:  78% (128/164)\u001b[K\rremote: Counting objects:  79% (130/164)\u001b[K\rremote: Counting objects:  80% (132/164)\u001b[K\rremote: Counting objects:  81% (133/164)\u001b[K\rremote: Counting objects:  82% (135/164)\u001b[K\rremote: Counting objects:  83% (137/164)\u001b[K\rremote: Counting objects:  84% (138/164)\u001b[K\rremote: Counting objects:  85% (140/164)\u001b[K\rremote: Counting objects:  86% (142/164)\u001b[K\rremote: Counting objects:  87% (143/164)\u001b[K\rremote: Counting objects:  88% (145/164)\u001b[K\rremote: Counting objects:  89% (146/164)\u001b[K\rremote: Counting objects:  90% (148/164)\u001b[K\rremote: Counting objects:  91% (150/164)\u001b[K\rremote: Counting objects:  92% (151/164)\u001b[K\rremote: Counting objects:  93% (153/164)\u001b[K\rremote: Counting objects:  94% (155/164)\u001b[K\rremote: Counting objects:  95% (156/164)\u001b[K\rremote: Counting objects:  96% (158/164)\u001b[K\rremote: Counting objects:  97% (160/164)\u001b[K\rremote: Counting objects:  98% (161/164)\u001b[K\rremote: Counting objects:  99% (163/164)\u001b[K\rremote: Counting objects: 100% (164/164)\u001b[K\rremote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 4151 (delta 33), reused 68 (delta 18), pack-reused 3987\u001b[K\n",
            "Receiving objects: 100% (4151/4151), 31.46 MiB | 22.32 MiB/s, done.\n",
            "Resolving deltas: 100% (1878/1878), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CvvfQ0RttAt9",
        "outputId": "c599a550-f757-49ce-95c9-1616e7f9b1a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%bash\n",
        "cd DeepLearningExamples\n",
        "git checkout master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your branch is up to date with 'origin/master'.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Already on 'master'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-rE46y-ftAuQ",
        "outputId": "db56336f-9975-455b-9308-17770b3ab7f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "\n",
        "WORKSPACE_DIR='/content/DeepLearningExamples/TensorFlow/Segmentation/UNet_Industrial/notebooks'\n",
        "os.chdir(WORKSPACE_DIR)\n",
        "print (os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepLearningExamples/TensorFlow/Segmentation/UNet_Industrial/notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b2vTOWtPuIE",
        "colab_type": "code",
        "outputId": "dda71789-db6a-4a48-bff4-f977ac41fc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!pip install tensorflow_hub==0.6.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.6.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.6.0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.6.0) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow_hub==0.6.0) (41.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HqSUGePjmTb9"
      },
      "source": [
        "## Data download\n",
        "\n",
        "We will first download some data for testing purposes, in particular, the [Weakly Supervised Learning for Industrial Optical Inspection (DAGM 2007)](https://resources.mpi-inf.mpg.de/conference/dagm/2007/prizes.html) dataset. \n",
        "\n",
        "> The competition is inspired by problems from industrial image processing. In order to satisfy their customers' needs, companies have to guarantee the quality of their products, which can often be achieved only by inspection of the finished product. Automatic visual defect detection has the potential to reduce the cost of quality assurance significantly.\n",
        ">\n",
        "> The competitors have to design a stand-alone algorithm which is able to detect miscellaneous defects on various background textures.\n",
        ">\n",
        "> The particular challenge of this contest is that the algorithm must learn, without human intervention, to discern defects automatically from a weakly labeled (i.e., labels are not exact to the pixel level) training set, the exact characteristics of which are unknown at development time. During the competition, the programs have to be trained on new data without any human guidance.\n",
        "\n",
        "**Source:** https://resources.mpi-inf.mpg.de/conference/dagm/2007/prizes.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S2PR7weWmTcK",
        "colab": {}
      },
      "source": [
        "! ./download_and_preprocess_dagm2007_public.sh ./data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EQAIszkxmTcT"
      },
      "source": [
        "The final data directory should look like:\n",
        "\n",
        "```\n",
        "./data\n",
        "  raw_images\n",
        "      public\n",
        "          Class1\t    \n",
        "          Class2\t\n",
        "          Class3\t    \n",
        "          Class4\t\n",
        "          Class5\t    \n",
        "          Class6\n",
        "          Class1_def  \n",
        "          Class2_def\t\n",
        "          Class3_def  \n",
        "          Class4_def\t\n",
        "          Class5_def  \n",
        "          Class6_def\n",
        "      private\n",
        "  zip_files\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSztH-mf-6hY"
      },
      "source": [
        "Each data directory contains training images corresponding to one of the first 6 types of defects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjOVDdyZgbT8",
        "colab_type": "text"
      },
      "source": [
        "## Load  UNet TF-Hub modules from Google Drive (Optional)\n",
        "\n",
        "This step allow you to connect and load pretrained UNet TF-Hub modules from Google Drive. Execute the below cell to authorize Colab to access your Google Drive content, then copy the saved TF-Hub to Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zga0RLrjgg4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8y1092rjAvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"/content/gdrive/My Drive/NVIDIA\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O5ZBrY4zcwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls NVIDIA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VunoLwxYtkdU",
        "colab_type": "text"
      },
      "source": [
        "## Inference with TF-Hub module\n",
        "\n",
        "Next, we will load one of the pretrained UNet TF-Hub modules (corresponding to one of the 10 classes of the DAGM 2007 dataset) and carry out inference.\n",
        "\n",
        "In order to load TF Hub modules, there are several options:\n",
        "\n",
        "- Load from a local cache or directory\n",
        "\n",
        "- Load from a remote repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doG457_Ut4qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "# Loading from a local cache/directory\n",
        "#module = hub.Module(\"NVIDIA/Unet/Class_1\", trainable=False)\n",
        "\n",
        "# Loading from a remote repository. The 10 NVIDIA UNet TF-Hub modules are available at\n",
        "# https://developer.download.nvidia.com/compute/redist/Binary_Files/unet_tfhub_modules/class_{1..10}\n",
        "module = hub.Module(\"https://developer.download.nvidia.com/compute/redist/Binary_Files/unet_tfhub_modules/class_1/1.tar.gz\")  # or class_2, class_3 etc..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw24AUjct-p7",
        "colab_type": "code",
        "outputId": "c3ed829f-44b5-4ed4-8d47-c828de6c7d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(module.get_signature_names())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['default']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKa0L82puB6q",
        "colab_type": "code",
        "outputId": "2c047a7f-f290-42f9-beaf-dd7743a6fd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(module.get_input_info_dict())   # When no signature is given, considers it as 'default'\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'images': <hub.ParsedTensorInfo shape=(?, 512, 512, 1) dtype=float32 is_sparse=False>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huUOMsfJuRd9",
        "colab_type": "code",
        "outputId": "a6d0ea6b-778c-490a-d305-c591244aed45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(module.get_output_info_dict())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'default': <hub.ParsedTensorInfo shape=(?, 512, 512, 1) dtype=float32 is_sparse=False>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJBWiPsPuX9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a test image\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "img = mpimg.imread('./data/raw_images/public/Class1_def/1.png')\n",
        "\n",
        "plt.figure(figsize = (10,10));\n",
        "plt.imshow(img, cmap='gray');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqaxvR09KYrr",
        "colab_type": "text"
      },
      "source": [
        "As we can see in this figure, there exists a defective area in the top left corner. We will now start a TF session and carry out inference on the normalized test image with the loaded TF-Hub module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK_wZTa_udHk",
        "colab_type": "code",
        "outputId": "017727bc-ce47-4dee-ef86-5e06db892648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Image preprocessing\n",
        "img =  np.expand_dims(img, axis=2)\n",
        "img =  np.expand_dims(img, axis=0)\n",
        "img = (img-0.5)/0.5\n",
        "\n",
        "output = module(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTwuefRtuzCu",
        "colab_type": "code",
        "outputId": "a6e944ea-665b-4ce5-8bd9-9b861ddd11c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 512, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXgfSMuRvoeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        " \n",
        "with tf.Session() as sess:\n",
        "    sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    pred = sess.run(output)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi9THAABzEHc",
        "colab_type": "code",
        "outputId": "93fb9d64-9ac5-46a5-d79a-7af2bdb50c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "# Print out model predicted mask\n",
        "plt.figure(figsize = (10,10));\n",
        "plt.imshow(np.squeeze(pred), cmap='gray');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJCCAYAAADQsoPKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ycE4aicK5GW",
        "colab_type": "text"
      },
      "source": [
        "As expected, the TF-Hub module points out the correct defective area in this image. Please feel free to try out other defective images for Class 1 within `./data/raw_images/public/Class1_def/`, or load the other UNet modules and test data for other classes from 1 to 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8MG1scbK8Ez",
        "colab_type": "code",
        "outputId": "f375cc88-8c68-46bb-e6e9-8bb7960e94b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!ls ./data/raw_images/public/Class1_def/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.png  116.png  131.png  147.png  26.png  41.png  57.png  72.png  88.png\n",
            "101.png  117.png  132.png  148.png  27.png  42.png  58.png  73.png  89.png\n",
            "102.png  118.png  133.png  149.png  28.png  43.png  59.png  74.png  8.png\n",
            "103.png  119.png  134.png  14.png   29.png  44.png  5.png   75.png  90.png\n",
            "104.png  11.png   135.png  150.png  2.png   45.png  60.png  76.png  91.png\n",
            "105.png  120.png  136.png  15.png   30.png  46.png  61.png  77.png  92.png\n",
            "106.png  121.png  137.png  16.png   31.png  47.png  62.png  78.png  93.png\n",
            "107.png  122.png  138.png  17.png   32.png  48.png  63.png  79.png  94.png\n",
            "108.png  123.png  139.png  18.png   33.png  49.png  64.png  7.png   95.png\n",
            "109.png  124.png  13.png   19.png   34.png  4.png   65.png  80.png  96.png\n",
            "10.png\t 125.png  140.png  1.png    35.png  50.png  66.png  81.png  97.png\n",
            "110.png  126.png  141.png  20.png   36.png  51.png  67.png  82.png  98.png\n",
            "111.png  127.png  142.png  21.png   37.png  52.png  68.png  83.png  99.png\n",
            "112.png  128.png  143.png  22.png   38.png  53.png  69.png  84.png  9.png\n",
            "113.png  129.png  144.png  23.png   39.png  54.png  6.png   85.png  labels.txt\n",
            "114.png  12.png   145.png  24.png   3.png   55.png  70.png  86.png\n",
            "115.png  130.png  146.png  25.png   40.png  56.png  71.png  87.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g8MxXY5GmTc8"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we have walked through the process of loading a pretrained UNet-Industrial TF-Hub module and carrying out inference on a test image.\n",
        "## What's next\n",
        "Now it's time to try the UNet-Industrial TF Hub modules on your own data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0SS61owr2oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}